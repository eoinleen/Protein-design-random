{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHqJaNB/JQQ8mcu3+qumiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eoinleen/Protein-design-random/blob/main/Rank_BindCraft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ll3D2_WXhXgh",
        "outputId": "48b5ef07-8d7d-4348-8cbb-730960389898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ xlsxwriter already installed\n",
            "\n",
            "============================================================\n",
            "Please upload a BindCraft CSV (e.g. final_design_stats.csv)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-98a1c41a-be9d-4c23-b490-4fce1a0073e6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-98a1c41a-be9d-4c23-b490-4fce1a0073e6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving trajectory_stats.csv to trajectory_stats (4).csv\n",
            "\n",
            "✅ Loaded trajectory_stats (4).csv\n",
            "   - 35 designs\n",
            "   - 44 columns\n",
            "\n",
            "Column P identified as: 'Unrelaxed_Clashes'\n",
            "Clash-related columns found: ['Unrelaxed_Clashes', 'Relaxed_Clashes']\n",
            "✅ Using 'Unrelaxed_Clashes' for clash filtering (threshold: ≤5, weight: 3.0)\n",
            "\n",
            "Metrics found in data:\n",
            "   - 3/27 filter metrics present\n",
            "\n",
            "Calculating comprehensive scores...\n",
            "\n",
            "⚠️ 29 designs with >5 clashes will be heavily penalized\n",
            "\n",
            "Score distribution:\n",
            "  Best normalized score: 1.083\n",
            "  Median normalized score: -0.062\n",
            "  Worst normalized score: -7.798\n",
            "\n",
            "Creating comprehensive Excel report...\n",
            "✅ Created comprehensive report with 4 sheets\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_865c4a10-ae69-4b01-951b-3eb9821f8c5a\", \"bindcraft_comprehensive_analysis.xlsx\", 39581)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "COMPREHENSIVE ANALYSIS COMPLETE!\n",
            "============================================================\n",
            "\n",
            "Overall Statistics:\n",
            "  Total designs: 35\n",
            "  Metrics evaluated: 3\n",
            "  Average normalized score: -0.917\n",
            "\n",
            "Performance Distribution:\n",
            "  Top 10%: 3 designs\n",
            "  Top 25%: 5 designs\n",
            "  Top 50%: 9 designs\n",
            "  Bottom 50%: 18 designs\n",
            "\n",
            "Filter Status Breakdown:\n",
            "  EXCELLENT: 6 designs (avg score: 0.874)\n",
            "  NEAR_MISS_1: 29 designs (avg score: -1.288)\n",
            "\n",
            "⚠️ Designs penalized for high clashes (>5): 29\n",
            "\n",
            "Top 5 Performers:\n",
            " final_rank                   Design  normalized_score  violation_count score_tier  Unrelaxed_Clashes\n",
            "          1  AMPK_Pocket_l89_s720529          1.082857                0    Top 10%                  3\n",
            "          2 AMPK_Pocket_l108_s482734          0.924762                0    Top 10%                  4\n",
            "          3  AMPK_Pocket_l84_s830749          0.912381                0    Top 10%                  4\n",
            "          4 AMPK_Pocket_l102_s373164          0.876190                0    Top 25%                  5\n",
            "          5 AMPK_Pocket_l109_s828745          0.805714                0    Top 25%                  5\n",
            "\n",
            "✅ Downloaded: bindcraft_comprehensive_analysis.xlsx\n",
            "\n",
            "Excel report includes:\n",
            "  • Summary - Complete data with all metrics\n",
            "  • Metrics_Colored - Visual highlighting of all thresholds\n",
            "  • Top_Performers - Top 25% of designs\n",
            "  • Score_Analysis - Detailed scoring breakdown\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "BindCraft Near-Miss Design Analysis & Comprehensive Ranking\n",
        "===========================================================\n",
        "\n",
        "Purpose\n",
        "-------\n",
        "Analyze BindCraft design CSVs with comprehensive scoring based on all metrics.\n",
        "Each design receives a weighted score based on how far each metric is from its threshold.\n",
        "\n",
        "Scoring System\n",
        "--------------\n",
        "- Each metric contributes to a composite score\n",
        "- Metrics are normalized by their distance from threshold\n",
        "- Penalties for violations, bonuses for exceeding thresholds\n",
        "- Special penalty for >5 unrelaxed clashes\n",
        "\n",
        "Ranking\n",
        "-------\n",
        "Designs are ranked by:\n",
        "1) Exclusion of high clash designs (>5 clashes)\n",
        "2) Composite score based on ALL metrics\n",
        "3) Violation count as tiebreaker\n",
        "4) Individual metric performance\n",
        "\n",
        "Input\n",
        "-----\n",
        "- Upload a BindCraft CSV when prompted\n",
        "\n",
        "Output\n",
        "------\n",
        "- bindcraft_near_miss_analysis.xlsx with comprehensive scoring\n",
        "- Color-coded Excel sheets showing metric performance\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from google.colab import files\n",
        "import sys\n",
        "\n",
        "# Try to import/install xlsxwriter\n",
        "try:\n",
        "    import xlsxwriter\n",
        "    print(\"✅ xlsxwriter already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing xlsxwriter for Excel formatting...\")\n",
        "    !pip install xlsxwriter -q\n",
        "    import xlsxwriter\n",
        "    print(\"✅ xlsxwriter installed successfully\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 1. COMPREHENSIVE FILTER CRITERIA WITH WEIGHTS\n",
        "# ------------------------------------------------------------------\n",
        "filter_config = {\n",
        "    # Critical metrics (weight = 2.0)\n",
        "    \"Average_pLDDT\": {\"threshold\": 0.8, \"higher\": True, \"weight\": 2.0},\n",
        "    \"1_pLDDT\": {\"threshold\": 0.8, \"higher\": True, \"weight\": 2.0},\n",
        "    \"Average_Binder_pLDDT\": {\"threshold\": 0.8, \"higher\": True, \"weight\": 2.0},\n",
        "    \"Average_i_pTM\": {\"threshold\": 0.5, \"higher\": True, \"weight\": 2.0},\n",
        "    \"1_i_pTM\": {\"threshold\": 0.5, \"higher\": True, \"weight\": 2.0},\n",
        "    \"i_pTM\": {\"threshold\": 0.5, \"higher\": True, \"weight\": 2.0},\n",
        "\n",
        "    # Important structural metrics (weight = 1.5)\n",
        "    \"Average_pTM\": {\"threshold\": 0.55, \"higher\": True, \"weight\": 1.5},\n",
        "    \"1_pTM\": {\"threshold\": 0.55, \"higher\": True, \"weight\": 1.5},\n",
        "    \"Average_i_pAE\": {\"threshold\": 0.35, \"higher\": False, \"weight\": 1.5},\n",
        "    \"1_i_pAE\": {\"threshold\": 0.35, \"higher\": False, \"weight\": 1.5},\n",
        "    \"i_pAE\": {\"threshold\": 0.35, \"higher\": False, \"weight\": 1.5},\n",
        "    \"Average_Binder_RMSD\": {\"threshold\": 3.5, \"higher\": False, \"weight\": 1.5},\n",
        "\n",
        "    # Interface quality metrics (weight = 1.5)\n",
        "    \"Average_ShapeComplementarity\": {\"threshold\": 0.6, \"higher\": True, \"weight\": 1.5},\n",
        "    \"1_ShapeComplementarity\": {\"threshold\": 0.55, \"higher\": True, \"weight\": 1.5},\n",
        "    \"Average_n_InterfaceHbonds\": {\"threshold\": 3, \"higher\": True, \"weight\": 1.5},\n",
        "    \"Average_n_InterfaceResidues\": {\"threshold\": 7, \"higher\": True, \"weight\": 1.5},\n",
        "\n",
        "    # Energy metrics (weight = 1.2)\n",
        "    \"Average_dG\": {\"threshold\": 0, \"higher\": False, \"weight\": 1.2},\n",
        "    \"Average_Binder_Energy_Score\": {\"threshold\": 0, \"higher\": False, \"weight\": 1.2},\n",
        "    \"Average_dSASA\": {\"threshold\": 1, \"higher\": True, \"weight\": 1.2},\n",
        "\n",
        "    # Secondary metrics (weight = 1.0)\n",
        "    \"2_pLDDT\": {\"threshold\": 0.8, \"higher\": True, \"weight\": 1.0},\n",
        "    \"Average_Surface_Hydrophobicity\": {\"threshold\": 0.35, \"higher\": False, \"weight\": 1.0},\n",
        "    \"Average_n_InterfaceUnsatHbonds\": {\"threshold\": 4, \"higher\": False, \"weight\": 1.0},\n",
        "    \"Average_Binder_Loop%\": {\"threshold\": 90, \"higher\": False, \"weight\": 1.0},\n",
        "    \"Average_Hotspot_RMSD\": {\"threshold\": 6, \"higher\": False, \"weight\": 1.0},\n",
        "\n",
        "    # Penalty metrics (weight = 0.8)\n",
        "    \"Average_InterfaceAAs_K\": {\"threshold\": 3, \"higher\": False, \"weight\": 0.8},\n",
        "    \"Average_InterfaceAAs_M\": {\"threshold\": 3, \"higher\": False, \"weight\": 0.8},\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 2. LOAD CSV\n",
        "# ------------------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Please upload a BindCraft CSV (e.g. final_design_stats.csv)\")\n",
        "print(\"=\"*60)\n",
        "uploaded = files.upload()\n",
        "csv_name = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(io.BytesIO(uploaded[csv_name]))\n",
        "\n",
        "print(f\"\\n✅ Loaded {csv_name}\")\n",
        "print(f\"   - {len(df)} designs\")\n",
        "print(f\"   - {len(df.columns)} columns\")\n",
        "\n",
        "# Find the unrelaxed clashes column\n",
        "column_P = None\n",
        "if len(df.columns) > 15:\n",
        "    column_P = df.columns[15]  # Column P\n",
        "    print(f\"\\nColumn P identified as: '{column_P}'\")\n",
        "\n",
        "clash_columns = [col for col in df.columns if \"clash\" in col.lower() or \"unrelaxed\" in col.lower()]\n",
        "if clash_columns:\n",
        "    print(f\"Clash-related columns found: {clash_columns}\")\n",
        "    if column_P in clash_columns:\n",
        "        rank_unrelaxed_clashes = column_P\n",
        "    else:\n",
        "        rank_unrelaxed_clashes = clash_columns[0]\n",
        "else:\n",
        "    rank_unrelaxed_clashes = column_P\n",
        "\n",
        "if rank_unrelaxed_clashes:\n",
        "    filter_config[rank_unrelaxed_clashes] = {\"threshold\": 5, \"higher\": False, \"weight\": 3.0}  # High weight for clashes\n",
        "    print(f\"✅ Using '{rank_unrelaxed_clashes}' for clash filtering (threshold: ≤5, weight: 3.0)\")\n",
        "\n",
        "# Show metrics present\n",
        "print(\"\\nMetrics found in data:\")\n",
        "metrics_present = []\n",
        "for metric in filter_config.keys():\n",
        "    if metric in df.columns:\n",
        "        metrics_present.append(metric)\n",
        "print(f\"   - {len(metrics_present)}/{len(filter_config)} filter metrics present\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 3. COMPREHENSIVE SCORING SYSTEM\n",
        "# ------------------------------------------------------------------\n",
        "def calculate_metric_score(value, threshold, higher_better, weight):\n",
        "    \"\"\"\n",
        "    Calculate a score for a single metric based on its distance from threshold.\n",
        "    Returns a weighted score where:\n",
        "    - Positive scores = better than threshold\n",
        "    - Negative scores = worse than threshold\n",
        "    - Magnitude indicates how far from threshold\n",
        "    \"\"\"\n",
        "    if pd.isna(value):\n",
        "        return -weight * 2  # Heavy penalty for missing values\n",
        "\n",
        "    if higher_better:\n",
        "        # For metrics where higher is better\n",
        "        if value >= threshold:\n",
        "            # Bonus for exceeding threshold (capped at 2x threshold for normalization)\n",
        "            normalized = min((value - threshold) / max(threshold, 0.1), 1.0)\n",
        "            return weight * normalized\n",
        "        else:\n",
        "            # Penalty for being below threshold\n",
        "            normalized = (threshold - value) / max(threshold, 0.1)\n",
        "            return -weight * normalized\n",
        "    else:\n",
        "        # For metrics where lower is better\n",
        "        if value <= threshold:\n",
        "            # Bonus for being below threshold\n",
        "            if threshold <= 0:\n",
        "                # For negative thresholds (like dG)\n",
        "                normalized = min(abs(value - threshold) / max(abs(threshold), 1), 1.0)\n",
        "            else:\n",
        "                # For positive thresholds\n",
        "                normalized = min((threshold - value) / max(threshold, 0.1), 1.0)\n",
        "            return weight * normalized\n",
        "        else:\n",
        "            # Penalty for exceeding threshold\n",
        "            normalized = (value - threshold) / max(abs(threshold), 0.1)\n",
        "            return -weight * normalized\n",
        "\n",
        "def analyze_design_comprehensive(row):\n",
        "    \"\"\"Analyze each design with comprehensive scoring\"\"\"\n",
        "    violations = 0\n",
        "    failed = []\n",
        "    composite_score = 0\n",
        "    metric_scores = {}\n",
        "\n",
        "    for metric, crit in filter_config.items():\n",
        "        if metric not in row.index:\n",
        "            continue\n",
        "\n",
        "        val = row[metric]\n",
        "        thresh = crit[\"threshold\"]\n",
        "        higher = crit[\"higher\"]\n",
        "        weight = crit.get(\"weight\", 1.0)\n",
        "\n",
        "        # Calculate metric score\n",
        "        score = calculate_metric_score(val, thresh, higher, weight)\n",
        "        metric_scores[metric] = score\n",
        "        composite_score += score\n",
        "\n",
        "        # Track violations\n",
        "        if pd.isna(val):\n",
        "            violations += 1\n",
        "            failed.append(f\"{metric}(NaN)\")\n",
        "        elif (higher and val < thresh) or (not higher and val > thresh):\n",
        "            violations += 1\n",
        "            failed.append(f\"{metric}({val:.3f})\")\n",
        "\n",
        "    return pd.Series([\n",
        "        violations,\n",
        "        \", \".join(failed),\n",
        "        composite_score,\n",
        "        len(metric_scores)  # Number of metrics evaluated\n",
        "    ])\n",
        "\n",
        "print(\"\\nCalculating comprehensive scores...\")\n",
        "df[[\"violation_count\", \"failed_metrics\", \"composite_score\", \"metrics_evaluated\"]] = \\\n",
        "    df.apply(analyze_design_comprehensive, axis=1)\n",
        "\n",
        "# Normalize composite score by number of metrics evaluated\n",
        "df[\"normalized_score\"] = df.apply(\n",
        "    lambda x: x[\"composite_score\"] / x[\"metrics_evaluated\"] if x[\"metrics_evaluated\"] > 0 else -999,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 4. CLASSIFICATION WITH SCORE RANGES\n",
        "# ------------------------------------------------------------------\n",
        "df[\"passes_all_filters\"] = df[\"violation_count\"] == 0\n",
        "\n",
        "def classify_with_score(row):\n",
        "    \"\"\"Classify based on violations and score\"\"\"\n",
        "    v = row[\"violation_count\"]\n",
        "    score = row[\"normalized_score\"]\n",
        "\n",
        "    if v == 0:\n",
        "        if score > 0.5:\n",
        "            return \"EXCELLENT\"\n",
        "        elif score > 0.2:\n",
        "            return \"PASS\"\n",
        "        else:\n",
        "            return \"PASS_MARGINAL\"\n",
        "    elif v == 1:\n",
        "        return \"NEAR_MISS_1\"\n",
        "    elif v == 2:\n",
        "        return \"NEAR_MISS_2\"\n",
        "    else:\n",
        "        return \"NEAR_MISS_3+\"\n",
        "\n",
        "df[\"filter_status\"] = df.apply(classify_with_score, axis=1)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 5. ADVANCED RANKING WITH COMPREHENSIVE SCORING\n",
        "# ------------------------------------------------------------------\n",
        "# Create high clash flag\n",
        "if rank_unrelaxed_clashes and rank_unrelaxed_clashes in df.columns:\n",
        "    df[\"high_clash_flag\"] = df[rank_unrelaxed_clashes].apply(\n",
        "        lambda x: 1 if pd.notna(x) and x > 5 else 0\n",
        "    )\n",
        "\n",
        "    high_clash_count = len(df[df[\"high_clash_flag\"] == 1])\n",
        "    if high_clash_count > 0:\n",
        "        print(f\"\\n⚠️ {high_clash_count} designs with >5 clashes will be heavily penalized\")\n",
        "else:\n",
        "    df[\"high_clash_flag\"] = 0\n",
        "\n",
        "# Calculate percentile ranks for key metrics\n",
        "key_metrics = [\"composite_score\", \"normalized_score\", \"violation_count\"]\n",
        "for metric in key_metrics:\n",
        "    if metric in df.columns:\n",
        "        df[f\"{metric}_percentile\"] = df[metric].rank(\n",
        "            ascending=(metric == \"violation_count\"),\n",
        "            pct=True\n",
        "        ) * 100\n",
        "\n",
        "print(\"\\nScore distribution:\")\n",
        "print(f\"  Best normalized score: {df['normalized_score'].max():.3f}\")\n",
        "print(f\"  Median normalized score: {df['normalized_score'].median():.3f}\")\n",
        "print(f\"  Worst normalized score: {df['normalized_score'].min():.3f}\")\n",
        "\n",
        "# Multi-level sorting\n",
        "sort_cols = [\n",
        "    \"high_clash_flag\",      # Exclude high clash designs\n",
        "    \"normalized_score\",      # Primary: normalized composite score\n",
        "    \"violation_count\",       # Secondary: number of violations\n",
        "    \"composite_score\"        # Tertiary: raw composite score\n",
        "]\n",
        "sort_asc = [True, False, True, False]\n",
        "\n",
        "# Add specific metric columns if available\n",
        "for metric in [\"i_pTM\", \"1_i_pTM\", \"Average_i_pTM\", \"Binder_pLDDT\", \"Average_Binder_pLDDT\"]:\n",
        "    if metric in df.columns:\n",
        "        sort_cols.append(metric)\n",
        "        sort_asc.append(False)\n",
        "\n",
        "# Apply sorting\n",
        "df = df.sort_values(by=sort_cols, ascending=sort_asc).reset_index(drop=True)\n",
        "\n",
        "# Add final rank\n",
        "df.insert(0, \"final_rank\", df.index + 1)\n",
        "\n",
        "# Add score tier\n",
        "def assign_tier(rank, total):\n",
        "    percentile = (rank / total) * 100\n",
        "    if percentile <= 10:\n",
        "        return \"Top 10%\"\n",
        "    elif percentile <= 25:\n",
        "        return \"Top 25%\"\n",
        "    elif percentile <= 50:\n",
        "        return \"Top 50%\"\n",
        "    else:\n",
        "        return \"Bottom 50%\"\n",
        "\n",
        "df.insert(1, \"score_tier\", df.apply(\n",
        "    lambda x: assign_tier(x[\"final_rank\"], len(df)), axis=1\n",
        "))\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 6. EXCEL OUTPUT WITH COMPREHENSIVE FORMATTING\n",
        "# ------------------------------------------------------------------\n",
        "try:\n",
        "    output_file = \"bindcraft_comprehensive_analysis.xlsx\"\n",
        "    print(\"\\nCreating comprehensive Excel report...\")\n",
        "\n",
        "    with pd.ExcelWriter(output_file, engine=\"xlsxwriter\") as writer:\n",
        "        workbook = writer.book\n",
        "\n",
        "        # Define formats\n",
        "        formats = {\n",
        "            \"red\": workbook.add_format({\"bg_color\": \"#FFC7CE\", \"font_color\": \"#9C0006\"}),\n",
        "            \"green\": workbook.add_format({\"bg_color\": \"#C6EFCE\", \"font_color\": \"#006100\"}),\n",
        "            \"yellow\": workbook.add_format({\"bg_color\": \"#FFEB9C\", \"font_color\": \"#9C5700\"}),\n",
        "            \"blue\": workbook.add_format({\"bg_color\": \"#DAE8FC\", \"font_color\": \"#00509E\"}),\n",
        "            \"dark_red\": workbook.add_format({\"bg_color\": \"#FF6B6B\", \"font_color\": \"#FFFFFF\", \"bold\": True}),\n",
        "            \"dark_green\": workbook.add_format({\"bg_color\": \"#4CAF50\", \"font_color\": \"#FFFFFF\", \"bold\": True}),\n",
        "            \"light_blue\": workbook.add_format({\"bg_color\": \"#E3F2FD\", \"font_color\": \"#1976D2\"}),\n",
        "        }\n",
        "\n",
        "        # Write sheets\n",
        "        sheets_config = [\n",
        "            (\"Summary\", df),\n",
        "            (\"Metrics_Colored\", df),\n",
        "            (\"Top_Performers\", df[df[\"score_tier\"].isin([\"Top 10%\", \"Top 25%\"])]),\n",
        "            (\"Score_Analysis\", df[[\"final_rank\", \"Design\", \"normalized_score\", \"composite_score\",\n",
        "                                   \"violation_count\", \"filter_status\", \"score_tier\"]])\n",
        "        ]\n",
        "\n",
        "        for sheet_name, sheet_df in sheets_config:\n",
        "            sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "            worksheet = writer.sheets[sheet_name]\n",
        "\n",
        "            # Apply conditional formatting\n",
        "            first_row = 1\n",
        "            last_row = len(sheet_df)\n",
        "\n",
        "            # Format all metric columns\n",
        "            for metric, crit in filter_config.items():\n",
        "                if metric not in sheet_df.columns:\n",
        "                    continue\n",
        "\n",
        "                col_idx = sheet_df.columns.get_loc(metric)\n",
        "                thresh = crit[\"threshold\"]\n",
        "                higher = crit[\"higher\"]\n",
        "\n",
        "                if higher:\n",
        "                    # Green for good, red for bad\n",
        "                    worksheet.conditional_format(\n",
        "                        first_row, col_idx, last_row, col_idx,\n",
        "                        {\"type\": \"cell\", \"criteria\": \">=\", \"value\": thresh, \"format\": formats[\"green\"]}\n",
        "                    )\n",
        "                    worksheet.conditional_format(\n",
        "                        first_row, col_idx, last_row, col_idx,\n",
        "                        {\"type\": \"cell\", \"criteria\": \"<\", \"value\": thresh, \"format\": formats[\"red\"]}\n",
        "                    )\n",
        "                else:\n",
        "                    # Green for good, red for bad\n",
        "                    worksheet.conditional_format(\n",
        "                        first_row, col_idx, last_row, col_idx,\n",
        "                        {\"type\": \"cell\", \"criteria\": \"<=\", \"value\": thresh, \"format\": formats[\"green\"]}\n",
        "                    )\n",
        "                    worksheet.conditional_format(\n",
        "                        first_row, col_idx, last_row, col_idx,\n",
        "                        {\"type\": \"cell\", \"criteria\": \">\", \"value\": thresh, \"format\": formats[\"red\"]}\n",
        "                    )\n",
        "\n",
        "            # Format score columns\n",
        "            if \"normalized_score\" in sheet_df.columns:\n",
        "                score_col = sheet_df.columns.get_loc(\"normalized_score\")\n",
        "                worksheet.conditional_format(\n",
        "                    first_row, score_col, last_row, score_col,\n",
        "                    {\"type\": \"3_color_scale\",\n",
        "                     \"min_color\": \"#FF0000\",\n",
        "                     \"mid_color\": \"#FFFF00\",\n",
        "                     \"max_color\": \"#00FF00\"}\n",
        "                )\n",
        "\n",
        "            # Format tier column\n",
        "            if \"score_tier\" in sheet_df.columns:\n",
        "                tier_col = sheet_df.columns.get_loc(\"score_tier\")\n",
        "                worksheet.conditional_format(\n",
        "                    first_row, tier_col, last_row, tier_col,\n",
        "                    {\"type\": \"text\", \"criteria\": \"containing\", \"value\": \"Top 10%\", \"format\": formats[\"dark_green\"]}\n",
        "                )\n",
        "                worksheet.conditional_format(\n",
        "                    first_row, tier_col, last_row, tier_col,\n",
        "                    {\"type\": \"text\", \"criteria\": \"containing\", \"value\": \"Top 25%\", \"format\": formats[\"green\"]}\n",
        "                )\n",
        "\n",
        "            # Format status column\n",
        "            if \"filter_status\" in sheet_df.columns:\n",
        "                status_col = sheet_df.columns.get_loc(\"filter_status\")\n",
        "                worksheet.conditional_format(\n",
        "                    first_row, status_col, last_row, status_col,\n",
        "                    {\"type\": \"text\", \"criteria\": \"containing\", \"value\": \"EXCELLENT\", \"format\": formats[\"dark_green\"]}\n",
        "                )\n",
        "                worksheet.conditional_format(\n",
        "                    first_row, status_col, last_row, status_col,\n",
        "                    {\"type\": \"text\", \"criteria\": \"containing\", \"value\": \"PASS\", \"format\": formats[\"green\"]}\n",
        "                )\n",
        "                worksheet.conditional_format(\n",
        "                    first_row, status_col, last_row, status_col,\n",
        "                    {\"type\": \"text\", \"criteria\": \"containing\", \"value\": \"NEAR_MISS\", \"format\": formats[\"yellow\"]}\n",
        "                )\n",
        "\n",
        "            # Auto-adjust column widths\n",
        "            for idx, column in enumerate(sheet_df.columns):\n",
        "                try:\n",
        "                    max_len = min(sheet_df[column].astype(str).map(len).max() + 2, 50)\n",
        "                    max_len = max(max_len, len(column) + 2)\n",
        "                    worksheet.set_column(idx, idx, max_len)\n",
        "                except:\n",
        "                    worksheet.set_column(idx, idx, 15)\n",
        "\n",
        "        print(f\"✅ Created comprehensive report with {len(sheets_config)} sheets\")\n",
        "\n",
        "    files.download(output_file)\n",
        "    excel_success = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"⚠️ Excel creation failed: {str(e)}\")\n",
        "    print(\"Falling back to CSV output...\")\n",
        "    excel_success = False\n",
        "\n",
        "    output_file = \"bindcraft_comprehensive_analysis.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    files.download(output_file)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7. COMPREHENSIVE SUMMARY STATISTICS\n",
        "# ------------------------------------------------------------------\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"COMPREHENSIVE ANALYSIS COMPLETE!\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "print(f\"\\nOverall Statistics:\")\n",
        "print(f\"  Total designs: {len(df)}\")\n",
        "print(f\"  Metrics evaluated: {len(metrics_present)}\")\n",
        "print(f\"  Average normalized score: {df['normalized_score'].mean():.3f}\")\n",
        "\n",
        "print(f\"\\nPerformance Distribution:\")\n",
        "for tier in [\"Top 10%\", \"Top 25%\", \"Top 50%\", \"Bottom 50%\"]:\n",
        "    count = len(df[df[\"score_tier\"] == tier])\n",
        "    print(f\"  {tier}: {count} designs\")\n",
        "\n",
        "print(f\"\\nFilter Status Breakdown:\")\n",
        "for status in df[\"filter_status\"].unique():\n",
        "    count = len(df[df[\"filter_status\"] == status])\n",
        "    avg_score = df[df[\"filter_status\"] == status][\"normalized_score\"].mean()\n",
        "    print(f\"  {status}: {count} designs (avg score: {avg_score:.3f})\")\n",
        "\n",
        "if rank_unrelaxed_clashes and \"high_clash_flag\" in df.columns:\n",
        "    high_clash = df[df[\"high_clash_flag\"] == 1]\n",
        "    if len(high_clash) > 0:\n",
        "        print(f\"\\n⚠️ Designs penalized for high clashes (>5): {len(high_clash)}\")\n",
        "\n",
        "print(f\"\\nTop 5 Performers:\")\n",
        "top_cols = [\"final_rank\", \"Design\", \"normalized_score\", \"violation_count\", \"score_tier\"]\n",
        "if rank_unrelaxed_clashes and rank_unrelaxed_clashes in df.columns:\n",
        "    top_cols.append(rank_unrelaxed_clashes)\n",
        "print(df[top_cols].head(5).to_string(index=False))\n",
        "\n",
        "if excel_success:\n",
        "    print(f\"\\n✅ Downloaded: {output_file}\")\n",
        "    print(\"\\nExcel report includes:\")\n",
        "    print(\"  • Summary - Complete data with all metrics\")\n",
        "    print(\"  • Metrics_Colored - Visual highlighting of all thresholds\")\n",
        "    print(\"  • Top_Performers - Top 25% of designs\")\n",
        "    print(\"  • Score_Analysis - Detailed scoring breakdown\")\n",
        "else:\n",
        "    print(f\"\\n✅ Downloaded: {output_file} (CSV format)\")"
      ]
    }
  ]
}